---
layout: post
title: 'IonTorrent: longer reads, longer homopolymers?'
date: 2011-08-10 15:56:09.000000000 +02:00
type: post
published: true
status: publish
categories:
- Bioinformatics
- Next Generation Sequencing
tags:
- '454'
- homopolymer
- Ion Torrent
meta:
  _edit_last: '12058266'
  twitter_cards_summary_img_size: a:7:{i:0;i:1504;i:1;i:792;i:2;i:2;i:3;s:25:"width="1504"
    height="792"";s:4:"bits";i:8;s:8:"channels";i:3;s:4:"mime";s:10:"image/jpeg";}
  _oembed_053adb3618beae7aafa025c59cd3a9b5: '{{unknown}}'
  _oembed_c792534bd14992a8b687be35c5db0c4e: '{{unknown}}'
  _oembed_1b760645a92501214e0e1132521d25d1: '{{unknown}}'
  _oembed_1484615213129667da56e7aff4d3b255: '{{unknown}}'
author:
  login: lexnederbragt
  email: lex.nederbragt@bio.uio.no
  display_name: lexnederbragt
  first_name: ''
  last_name: ''
---
<p><em>A quick summary for the impatient:</em></p>
<p>An analysis of the homopolymer distribution of the recently released 'longer' Ion Torrent reads indicates a possible significant over-calling of homopolymer lengths towards the ends of the reads. Trimming the ends off, however, only marginally improved <em>de novo</em> assembly of the reads using newbler.</p>
<p>Life recently released 'long' IonTorrent reads (B14_387, resequencing of <em>E coli</em> strain DH10B, available through the Ion Community <a href="http://lifetech-it.hosted.jivesoftware.com/docs/DOC-1848">here</a>). There is an accompanying <a href="http://www.iontorrent.com/applications-pgm-accuracy/">application note</a> that brags about the read's accuracy, especially over reads from the MiSeq platform. These accuracy measurements are logically based on alignment to a reference genome.</p>
<p>But what about <em>de novo</em> assembly? Thing is, the dataset presented, with a peak length of 241 (see below) and 350 000 read, is quite similar to what a full plate of GS FLX gave you in 2007 (peak length 250 bases, 400 000 reads). And 454 reads were very useful at the time for <em>de novo</em> assembly (in fact, the only reads available for this purpose, obviously besides Sanger reads).</p>
<p><!--more-->Nick Loman had a stab at using these reads for assembly with newbler, the program developed by 454 Life Sciences, on <a href="http://pathogenomics.bham.ac.uk/blog/2011/08/ion-torrent-what-is-the-impact-of-the-new-longer-reads-on-assembly/">his blog</a>. He reported, compared to the shorter Ion reads, much worse results for the longer reads. <a href="https://twitter.com/#%21/pathogenomenick/status/100597152933363712">On twitter</a>, he wrote that the assembly programs MIRA and CLC Bio performed really bad with these reads.</p>
<p>So I wondered what could be the problem with these reads, that presumably show good mapping accuracy, but perform not very well for assembly. For comparison, I generated a similar 454 GS FLX dataset, 350 000 reads with a peak length at 250 bases. I used sff files that came with the software installation discs of newbler version 1.1, these represent a resequencing run on <em>E coli</em> K12 strain MG1655. I randomly selected at the exact number of reads as are in the Ion Torrent B14_387 dataset.</p>
<p>First, I looked into the sff file and noticed that the run had 520 flows, twice that was used earlier. Using my '2.5 bases per four flows' calculation <a href="/2011/06/16/there-is-more-length-to-ion-torrent-reads-than-meets-the-eye-and-is-ion-torrent-hiding-it-2/ ">I described before</a>, this translates in a potential 325 bases. <del>The flow order used was reported as TACG all the way.</del> In a run <a href="/2011/07/01/going-with-the-flow-sequencing-flow-orders-and-longer-reads-ion-sets-the-stage/">I discussed earlier</a> , I noticed a 32 base repeated flow order which presumably yields better results, <del>but</del> and this flow order was <del>apparently not</del> used for this run as well [thanks to Nils Homer for pointing out my oversight here, I've ordered new contact lenses...).</p>
<p>I then took a closer look at the read length distribution of both datasets</p>
<p><a href="http://flxlexblog.files.wordpress.com/2011/08/b14_etc_readlen.jpg"><img class="alignnone size-full wp-image-134" title="B14_etc_readlen" src="{{ site.baseurl }}/assets/b14_etc_readlen.jpg" alt="" width="480" height="252" /></a></p>
<p>The Ion reads peak slightly lower than the 454 reads (241 and 253, respectively). But, they also show a strange peak around 400 bases. On the Ion Community forums, this peaks was discussed, but I couldn't find a satisfactory explanation for it.</p>
<p>For problems with the data generated by both 454 and Ion Torrent, the usual suspect is homopolymers, consecutive runs of the same bases. Due to the way the bases are read (basically not reading single bases, but the length of homopolymers). I calculated the frequencies of homopolymers of all lengths present in both the Ion long reads dataset, the B13_328 dataset (100 bases run), the 454 dataset, and the E coli K12 MG1655 genome (both forward and reverse strands). (The results for DH10B were identical to MG1655 and are therefore not shown), see the graph.</p>
<p><a href="http://flxlexblog.files.wordpress.com/2011/08/b14_etc_homopol_counts.jpg"><img class="alignnone size-full wp-image-127" title="B14_etc_homopol_counts" src="{{ site.baseurl }}/assets/b14_etc_homopol_counts.jpg" alt="" width="480" height="286" /></a></p>
<p>In the figure, the counts of the homopolymers of lengths 1-11 are plotted, with the Y-axis on a log scale. A homopolymer length of 1 represent all single bases (monomers), length 2 represents all occurrences of AA, CC, GG and TT, length 3 represents all AAA, CCC, GGG and TTTs, etc.</p>
<p>The line representing the E coli genome (the lowest one due to the 2x coverage, while the run data have much higher coverage) showed a steady (exponential) decline in the counts, accelerating beyond 8-mers. The 454 reads nicely follow this line and showed the same pattern. The Ion reads, on the other hand, deviated significantly. The showed a slowing down in the decline beyond 8-mers. So, it looks like the Ion reads have significantly more longer homopolymers than expected based on the composition of the genome. This could mean that there is a profound over-calling of homopolymers in these reads.</p>
<p>(As an aside, the Ion B14_387 long reads had no homopolymers beyond 11-mers, the B13_328 short run had a few above that, up to 23-mers, the 454 data showed very few above 10-mers and maxed out at a whopping 31-mers (artifacts, perhaps?). The E coli genome did not have longer homopolymers than 10-mers.)</p>
<p>I next tested whether these longer homopolymers were located uniformly along the reads, or where perhaps clustered more towards the ends. To this end, I counted the starting positions for all homopolymers of each read, for the 454 and B14 long read datasets.</p>
<p><a href="http://flxlexblog.files.wordpress.com/2011/08/454_homopol_pos.jpg"><img class="alignnone size-full wp-image-128" title="454_homopol_pos" src="{{ site.baseurl }}/assets/454_homopol_pos.jpg" alt="" width="480" height="295" /></a></p>
<p>The 454 reads, above, showed a uniform distribution of homopolymers of the different lengths along the read up to about 320 bases. The pattern for the last bases is a result of the very few reads that are of this length, so that N is very low for these positions.</p>
<p><a href="http://flxlexblog.files.wordpress.com/2011/08/b14_387_homopol_pos.jpg"><img class="alignnone size-full wp-image-129" title="B14_387_homopol_pos" src="{{ site.baseurl }}/assets/b14_387_homopol_pos.jpg" alt="" width="480" height="282" /></a></p>
<p>The long Ion reads, on the other hand, show a significant reduction of the fraction of 1-mers beyond base 230, and an increase of the 2-mer and longer homopolymers. The very long reads (the peak around 400 nt) show the opposite, a reduction in the number of homopolymers in favor of monomers.</p>
<p>In an analysis of the test fragments spiked into the IonTorrent runs, lek2k showed on his biolektures blog (posts <a href="http://biolektures.wordpress.com/2011/08/01/ion-torrent-test-fragment-accuracy-part-1/">here</a> and <a href="http://biolektures.wordpress.com/2011/08/05/ion-torrent-%E2%80%93-test-fragment-accuracy-part-2/">here</a>) both undercalls and overcalls for two-mers in the test fragments <del>for longer reads</del> deep into the reads [thanks to lek2k for correcting me on this, see his interesting comment below]. My results indicates that overcalls are the bigger of the two problems.</p>
<p>These analysis indicate (but in no way prove) that homopolymer overcalls could contribute significantly to a lower accuracy at the end of the longer IonTorrent reads. Is this problem then explaining the low assembly performance of these reads?</p>
<p>In order to test this, I created new sff files for both B14_387 and the 454 dataset, with the reads trimmed beyond 230 bases. Then, I performed assemblies for the original files, and the retrimmed files using newbler 2.6 with default settings.</p>
<p><a href="http://flxlexblog.files.wordpress.com/2011/08/b14_etc_asm_table.jpg"><img class="alignnone size-full wp-image-130" title="B14_etc_asm_table" src="{{ site.baseurl }}/assets/b14_etc_asm_table.jpg" alt="" width="480" height="189" /></a></p>
<p>('Large' contigs are those of at least 500 bp). The table shows not that much difference between the full-length 454 reads, and those trimmed at 230 bases, with only a few more contigs and a 20% lower N50. for the latter assembly.Â  The full-length Ion reads assembly is similar to the one Nick Loman produced, three times more contigs and an N50 that is a fifth of the full-length 454 assembly. Note that many reads are only partially assembled (meaning that a part of them could be used for alignment against the other reads, the rest was too different), whereas close to 99% of the 454 reads are fully assembled. Finally, the assembly using the trimmed Ion reads is only marginally better.</p>
<p>In conclusion, these results indicate that I have found a problem, but that it most likely is not fully able to explain the poor performance of the long Ion reads for assembly. Also, the GS FLX reads (peak length at 250), around for more than four years now, perform significantly better than Ion reads of comparable length for<em> de novo</em> assembly.</p>
<p>Finally, I'd be interested to see an analysis of the homopolymer errors based on mapping the reads to the reference genome for both these datasets. Since this is something that would take me way too much time, I hope somebody else will do this analysis (I can help if needed, and you're welcome to write a guest post on my blog if you want)!</p>
<p><strong>Code used</strong><br />
Randomly picking 350109 reads from the 454 sff files to get the same number as the IonTorrent B14_387 run<br />
sfffile -pickr 350109 -o EBO6PME_350k_reads.sff EBO6PME0*.sff</p>
<p><span style="text-decoration:underline;">Readlength distributions</span><br />
(see also <a href="/2011/06/16/there-is-more-length-to-ion-torrent-reads-than-meets-the-eye-and-is-ion-torrent-hiding-it-2/">this post</a>):<br />
sffinfo -s reads.sff|fasta_length | awk '{x[$1]++}END{for (i in x){print i"\t"x[i]}}'| sort -n &gt;read_length_hist.tsv</p>
<p><span style="text-decoration:underline;">Homopolymer counts</span><br />
I used the following script (thanks to 'Deep' for pointing out a bug in this script, see the Comments section. The results changed slightly after fixing the bug but this did not influence the conclusions. Below is the correct version):</p>
<p>[sourcecode language="css"]#!/usr/bin/perl</p>
<p>use strict;<br />
use warnings;</p>
<p>my $seq;<br />
my $seen;Â Â  Â Â Â  Â # for tracking homopolymers<br />
my $i;Â Â  Â Â Â  Â Â Â  Â # running length of homopolymer<br />
my $max=0;Â Â  Â Â Â  Â # longest homopolymer found<br />
my @hompol;Â Â  Â Â Â  Â # hompol = homopolymer counts, e.g. hompol[&quot;A&quot;][3] is the count of AAA<br />
my %bases; $bases{&quot;A&quot;} = 1;$bases{&quot;C&quot;} = 2;$bases{&quot;G&quot;} = 3;$bases{&quot;T&quot;} = 4;</p>
<p># set the record separator to the '&amp;gt;' symbol<br />
$/=&quot;&gt;&quot;;<br />
&lt;&gt;;Â Â  Â Â Â  Â # remove the empty first 'sequence'<br />
# loop over the sequences<br />
while (&lt;&gt;){<br />
$seen = &quot;&quot;;<br />
$i=1;<br />
# remove the trailing '&gt;' symbol<br />
chomp;<br />
# split the entry into individual lines based on the newline character<br />
my @lines = split(/\n/,$_);<br />
# the header is the first line (now without the '&gt;' symbol)<br />
my $header = shift @lines;<br />
# the sequence is the rest<br />
my $seq = join &quot;&quot;, @lines;</p>
<p># loop over the bases<br />
for (split (//, uc($seq)))Â  {<br />
# skip N's<br />
next if /N/;<br />
# when the current base is the same as the previous one<br />
if ($seen eq $_){$i++}<br />
# otherwise, the homopolymer run has ended<br />
else {<br />
$hompol[$bases{$seen}][$i]++ if $seen ne &quot;&quot;;<br />
$max=$i if $i&gt;$max;<br />
$i=1;<br />
}<br />
$seen=$_;<br />
}<br />
# do not forget last homopolymer<br />
$hompol[$bases{$seen}][$i]++ if $seen ne &quot;&quot;;<br />
$max=$i if $i&gt;$max;<br />
}<br />
$/=&quot;\n&quot;; # reset the record separator</p>
<p># output<br />
print &quot;\tA\tC\tG\tT\n&quot;;<br />
# homopolymer length loop<br />
for $i (1..$max){<br />
print $i;<br />
# bases loop<br />
for my $j (1..4){<br />
print &quot;\t&quot;;<br />
print $hompol[$j][$i] || 0;<br />
}<br />
print &quot;\n&quot;;<br />
}<br />
[/sourcecode]</p>
<p>This produces a table with homopolymer length in rows, and bases in columns and the counts as values in the cells. I than (using excel) summed over the rows to get the totals, and plotted these in excel.</p>
<p><span style="text-decoration:underline;">Homopolymer starting positions</span><br />
For this, I used a modified version of the above scriptÂ (thanks to 'terry' for pointing out a bug in <em>this</em> script, see the Comments section. Again, the results changed very slightly after fixing the bug, and again this did not influence the conclusions. Below is the correct version):</p>
<p>[sourcecode language="css"]#!/usr/bin/perl<br />
use strict;<br />
use warnings;</p>
<p>my $seq;<br />
my $seen;Â Â  Â Â Â  Â # for tracking homopolymers<br />
my $i;Â Â  Â Â Â  Â Â Â  Â # running length of homopolymer<br />
my $pos;Â Â  Â Â Â  Â # running counter of base position<br />
my $max_hpol=0;Â Â  Â # maximum homopolyer length encountered<br />
my $max_len=0;Â Â  Â # maximum sequence length encountered<br />
my @poscounts; Â Â  Â # hash counting starting positions for all homopolymers; $poscounts[basepos][hompollength] = count</p>
<p>$/=&quot;&gt;&quot;; # set the record separator to the '&gt;' symbol<br />
&lt;&gt;;Â Â  Â Â Â  Â # remove the empty first 'sequence'<br />
# loop over the sequences<br />
while (&lt;&gt;){<br />
$seen = &quot;&quot;;<br />
$i=1;<br />
$pos = 0;<br />
chomp;Â Â  Â # remove the trailing '&gt;' symbol<br />
my @lines = split(/\n/,$_);Â Â  Â # split the entry into individual lines based on the newline character<br />
my $header = shift @lines;Â Â  Â # the header is the first line (now without the '&gt;' symbol)<br />
# the sequences is now the rest<br />
my $seq = join &quot;&quot;, @lines;<br />
$max_len = length ($seq) if length($seq) &gt; $max_len;</p>
<p># Loop over the bases<br />
for (split (//, uc($seq)))Â  {<br />
$pos++;<br />
# skip N's<br />
next if /N/;<br />
# when the current base is the same as the previous one<br />
if ($seen eq $_){$i++}<br />
# otherwise, the homopolymer run has ended<br />
else{<br />
# $pos is now position of current base, one beyond the last homopolymer<br />
# $i is length of last homopolymer<br />
# $pos-$i =Â  starting base of last homopolymer<br />
$poscounts[$pos-$i][$i]++ if $seen ne &quot;&quot;;<br />
$max_hpol=$i if $i&gt;$max_hpol;<br />
$i=1;<br />
}<br />
$seen=$_;<br />
}<br />
# do not forget last homopolymer<br />
$pos++;<br />
$poscounts[$pos-$i][$i]++ if $seen ne &quot;&quot;;</p>
<p>}<br />
$/=&quot;\n&quot;; # reset the record separator</p>
<p># output<br />
print join &quot;\t&quot;, (&quot;&quot;, 1..$max_hpol);<br />
print &quot;\n&quot;;<br />
# bases loop<br />
for $i (1..$max_len){<br />
print $i;<br />
for my $j (1..$max_hpol){<br />
# homopolymer length loop<br />
print &quot;\t&quot;;<br />
print $poscounts[$i][$j] || 0;<br />
}<br />
print &quot;\n&quot;;<br />
}<br />
[/sourcecode]</p>
<p>The output of this script is a table with the base positions in the rows, and the homopolymer lengths as columns, the counts as values in the cells.</p>
<p><span style="text-decoration:underline;">Resetting the trimpoints to 230 bases</span><br />
First, I created a trimpoint file:</p>
<blockquote><p><code>sffinfo R_2011_07_19_20_05_38_user_B14-387-r121336-314_pool30-ms_B14-387_cafie_0.05.sff|awk '{<br />
if ($0 ~ /&gt;/){id=$0}<br />
if ($0 ~/Clip Qual Right/){l=$4; if(l&gt;230){l=230}}<br />
if ($0 ~/Clip Adap Left/){print id" 5 "l}<br />
}' |sed 's/&gt;//' &gt;B14_387_cafie_0.05_new_trimpoints_max_230.txt</code></p></blockquote>
<p>Then, I used the sfffile command to generate a new sff file, with new right trimpoints for those reads that are over 230 bases:</p>
<blockquote><p><code>sfffile -oÂ  B14_387_cafie_0.05_new_trimpoints_max_230.sff -tr B14_387_cafie_0.05_new_trimpoints_max_230.txt R_2011_07_19_20_05_38_user_B14-387-r121336-314_pool30-ms_B14-387_cafie_0.05.sff</code></p></blockquote>
<p><span style="text-decoration:underline;">Assemblies</span><br />
All assemblies were done with newbler 2.6 with default parameters, e.g.</p>
<blockquote><p><code>runAssembly -o trim_at_230 B14_387_cafie_0.05_new_trimpoints_max_230.sff</code></p></blockquote>
<p>For the summary of the assembly statistics, I used my <a href="http://sourceforge.net/projects/newblertools/files/newblermetrics">newblermetrics script</a>. &gt; &lt;</p>
